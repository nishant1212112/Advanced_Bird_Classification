{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird Detection with Open CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\t\"sofa\", \"train\", \"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(\"MobileNetSSD_deploy.prototxt.txt\" , \"MobileNetSSD_deploy.caffemodel\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2531)\n"
     ]
    }
   ],
   "source": [
    "CLASSES1=[x for x in os.listdir('Images') if x!='.DS_Store']\n",
    "ImagePaths=[]\n",
    "for class1 in CLASSES1:\n",
    "    Folder=os.path.join('Images',class1)\n",
    "    ImagePaths= ImagePaths+[os.path.join(Folder,x) for x in os.listdir(Folder) if x!='.DS_Store']\n",
    "print(len(CLASSES1),len(ImagePaths))\n",
    "Images = [cv2.imread(image) for image in ImagePaths]\n",
    "Sizes=[image.shape for image in Images]\n",
    "Num_Images=len(ImagePaths)\n",
    "images=[cv2.resize(image,(300,300)) for image in Images]\n",
    "images=np.stack(images,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422, 3, 300, 300)\n",
      "[INFO] computing object detections...\n",
      "(422, 3, 300, 300)\n",
      "[INFO] computing object detections...\n",
      "(422, 3, 300, 300)\n",
      "[INFO] computing object detections...\n",
      "(422, 3, 300, 300)\n",
      "[INFO] computing object detections...\n",
      "(422, 3, 300, 300)\n",
      "[INFO] computing object detections...\n",
      "(421, 3, 300, 300)\n",
      "[INFO] computing object detections...\n"
     ]
    }
   ],
   "source": [
    "# pass the blob through the network and obtain the detections and\n",
    "# predictions\n",
    "for indices in np.array_split(range(Num_Images),(Num_Images/500)+1):\n",
    "    blob=cv2.dnn.blobFromImages(images[indices], 0.007843, (300, 300), 127.5)\n",
    "    print(blob.shape)\n",
    "    print(\"[INFO] computing object detections...\")\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on dnn_Net object:\n",
      "\n",
      "class dnn_Net(__builtin__.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
      " |  \n",
      " |  __repr__(...)\n",
      " |      x.__repr__() <==> repr(x)\n",
      " |  \n",
      " |  connect(...)\n",
      " |      connect(outPin, inpPin) -> None\n",
      " |      .   @brief Connects output of the first layer to input of the second layer.\n",
      " |      .   *  @param outPin descriptor of the first layer output.\n",
      " |      .   *  @param inpPin descriptor of the second layer input.\n",
      " |      .   *\n",
      " |      .   * Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:\n",
      " |      .   * - the first part of the template <DFN>layer_name</DFN> is sting name of the added layer.\n",
      " |      .   *   If this part is empty then the network input pseudo layer will be used;\n",
      " |      .   * - the second optional part of the template <DFN>input_number</DFN>\n",
      " |      .   *   is either number of the layer input, either label one.\n",
      " |      .   *   If this part is omitted then the first layer input will be used.\n",
      " |      .   *\n",
      " |      .   *  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()\n",
      " |  \n",
      " |  empty(...)\n",
      " |      empty() -> retval\n",
      " |      .   Returns true if there are no layers in the network.\n",
      " |  \n",
      " |  enableFusion(...)\n",
      " |      enableFusion(fusion) -> None\n",
      " |      .   @brief Enables or disables layer fusion in the network.\n",
      " |      .   * @param fusion true to enable the fusion, false to disable. The fusion is enabled by default.\n",
      " |  \n",
      " |  forward(...)\n",
      " |      forward([, outputName]) -> retval\n",
      " |      .   @brief Runs forward pass to compute output of layer with name @p outputName.\n",
      " |      .   *  @param outputName name for layer which output is needed to get\n",
      " |      .   *  @return blob for first output of specified layer.\n",
      " |      .   *  @details By default runs forward pass for the whole network.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      forward([, outputBlobs[, outputName]]) -> outputBlobs\n",
      " |      .   @brief Runs forward pass to compute output of layer with name @p outputName.\n",
      " |      .   *  @param outputBlobs contains all output blobs for specified layer.\n",
      " |      .   *  @param outputName name for layer which output is needed to get\n",
      " |      .   *  @details If @p outputName is empty, runs forward pass for the whole network.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      forward(outBlobNames[, outputBlobs]) -> outputBlobs\n",
      " |      .   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n",
      " |      .   *  @param outputBlobs contains blobs for first outputs of specified layers.\n",
      " |      .   *  @param outBlobNames names for layers which outputs are needed to get\n",
      " |  \n",
      " |  forwardAndRetrieve(...)\n",
      " |      forwardAndRetrieve(outBlobNames) -> outputBlobs\n",
      " |      .   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n",
      " |      .   *  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.\n",
      " |      .   *  @param outBlobNames names for layers which outputs are needed to get\n",
      " |  \n",
      " |  getFLOPS(...)\n",
      " |      getFLOPS(netInputShapes) -> retval\n",
      " |      .   @brief Computes FLOP for whole loaded model with specified input shapes.\n",
      " |      .   * @param netInputShapes vector of shapes for all net inputs.\n",
      " |      .   * @returns computed FLOP.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getFLOPS(netInputShape) -> retval\n",
      " |      .   @overload\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getFLOPS(layerId, netInputShapes) -> retval\n",
      " |      .   @overload\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getFLOPS(layerId, netInputShape) -> retval\n",
      " |      .   @overload\n",
      " |  \n",
      " |  getLayer(...)\n",
      " |      getLayer(layerId) -> retval\n",
      " |      .   @brief Returns pointer to layer with specified id or name which the network use.\n",
      " |  \n",
      " |  getLayerId(...)\n",
      " |      getLayerId(layer) -> retval\n",
      " |      .   @brief Converts string name of the layer to the integer identifier.\n",
      " |      .   *  @returns id of the layer, or -1 if the layer wasn't found.\n",
      " |  \n",
      " |  getLayerNames(...)\n",
      " |      getLayerNames() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  getLayerTypes(...)\n",
      " |      getLayerTypes() -> layersTypes\n",
      " |      .   @brief Returns list of types for layer used in model.\n",
      " |      .   * @param layersTypes output parameter for returning types.\n",
      " |  \n",
      " |  getLayersCount(...)\n",
      " |      getLayersCount(layerType) -> retval\n",
      " |      .   @brief Returns count of layers of specified type.\n",
      " |      .   * @param layerType type.\n",
      " |      .   * @returns count of layers\n",
      " |  \n",
      " |  getLayersShapes(...)\n",
      " |      getLayersShapes(netInputShapes) -> layersIds, inLayersShapes, outLayersShapes\n",
      " |      .   @brief Returns input and output shapes for all layers in loaded model;\n",
      " |      .   *  preliminary inferencing isn't necessary.\n",
      " |      .   *  @param netInputShapes shapes for all input blobs in net input layer.\n",
      " |      .   *  @param layersIds output parameter for layer IDs.\n",
      " |      .   *  @param inLayersShapes output parameter for input layers shapes;\n",
      " |      .   * order is the same as in layersIds\n",
      " |      .   *  @param outLayersShapes output parameter for output layers shapes;\n",
      " |      .   * order is the same as in layersIds\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getLayersShapes(netInputShape) -> layersIds, inLayersShapes, outLayersShapes\n",
      " |      .   @overload\n",
      " |  \n",
      " |  getMemoryConsumption(...)\n",
      " |      getMemoryConsumption(netInputShape) -> weights, blobs\n",
      " |      .   @overload\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getMemoryConsumption(layerId, netInputShapes) -> weights, blobs\n",
      " |      .   @overload\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      getMemoryConsumption(layerId, netInputShape) -> weights, blobs\n",
      " |      .   @overload\n",
      " |  \n",
      " |  getParam(...)\n",
      " |      getParam(layer[, numParam]) -> retval\n",
      " |      .   @brief Returns parameter blob of the layer.\n",
      " |      .   *  @param layer name or id of the layer.\n",
      " |      .   *  @param numParam index of the layer parameter in the Layer::blobs array.\n",
      " |      .   *  @see Layer::blobs\n",
      " |  \n",
      " |  getPerfProfile(...)\n",
      " |      getPerfProfile() -> retval, timings\n",
      " |      .   @brief Returns overall time for inference and timings (in ticks) for layers.\n",
      " |      .   * Indexes in returned vector correspond to layers ids. Some layers can be fused with others,\n",
      " |      .   * in this case zero ticks count will be return for that skipped layers.\n",
      " |      .   * @param timings vector for tick timings for all layers.\n",
      " |      .   * @return overall ticks for model inference.\n",
      " |  \n",
      " |  getUnconnectedOutLayers(...)\n",
      " |      getUnconnectedOutLayers() -> retval\n",
      " |      .   @brief Returns indexes of layers with unconnected outputs.\n",
      " |  \n",
      " |  getUnconnectedOutLayersNames(...)\n",
      " |      getUnconnectedOutLayersNames() -> retval\n",
      " |      .   @brief Returns names of layers with unconnected outputs.\n",
      " |  \n",
      " |  setHalideScheduler(...)\n",
      " |      setHalideScheduler(scheduler) -> None\n",
      " |      .   * @brief Compile Halide layers.\n",
      " |      .   * @param[in] scheduler Path to YAML file with scheduling directives.\n",
      " |      .   * @see setPreferableBackend\n",
      " |      .   *\n",
      " |      .   * Schedule layers that support Halide backend. Then compile them for\n",
      " |      .   * specific target. For layers that not represented in scheduling file\n",
      " |      .   * or if no manual scheduling used at all, automatic scheduling will be applied.\n",
      " |  \n",
      " |  setInput(...)\n",
      " |      setInput(blob[, name[, scalefactor[, mean]]]) -> None\n",
      " |      .   @brief Sets the new input value for the network\n",
      " |      .   *  @param blob        A new blob. Should have CV_32F or CV_8U depth.\n",
      " |      .   *  @param name        A name of input layer.\n",
      " |      .   *  @param scalefactor An optional normalization scale.\n",
      " |      .   *  @param mean        An optional mean subtraction values.\n",
      " |      .   *  @see connect(String, String) to know format of the descriptor.\n",
      " |      .   *\n",
      " |      .   *  If scale or mean values are specified, a final input blob is computed\n",
      " |      .   *  as:\n",
      " |      .   * \\f[input(n,c,h,w) = scalefactor \\times (blob(n,c,h,w) - mean_c)\\f]\n",
      " |  \n",
      " |  setInputsNames(...)\n",
      " |      setInputsNames(inputBlobNames) -> None\n",
      " |      .   @brief Sets outputs names of the network input pseudo layer.\n",
      " |      .   *\n",
      " |      .   * Each net always has special own the network input pseudo layer with id=0.\n",
      " |      .   * This layer stores the user blobs only and don't make any computations.\n",
      " |      .   * In fact, this layer provides the only way to pass user data into the network.\n",
      " |      .   * As any other layer, this layer can label its outputs and this function provides an easy way to do this.\n",
      " |  \n",
      " |  setParam(...)\n",
      " |      setParam(layer, numParam, blob) -> None\n",
      " |      .   @brief Sets the new value for the learned param of the layer.\n",
      " |      .   *  @param layer name or id of the layer.\n",
      " |      .   *  @param numParam index of the layer parameter in the Layer::blobs array.\n",
      " |      .   *  @param blob the new value.\n",
      " |      .   *  @see Layer::blobs\n",
      " |      .   *  @note If shape of the new blob differs from the previous shape,\n",
      " |      .   *  then the following forward pass may fail.\n",
      " |  \n",
      " |  setPreferableBackend(...)\n",
      " |      setPreferableBackend(backendId) -> None\n",
      " |      .   * @brief Ask network to use specific computation backend where it supported.\n",
      " |      .   * @param[in] backendId backend identifier.\n",
      " |      .   * @see Backend\n",
      " |      .   *\n",
      " |      .   * If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT\n",
      " |      .   * means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV.\n",
      " |  \n",
      " |  setPreferableTarget(...)\n",
      " |      setPreferableTarget(targetId) -> None\n",
      " |      .   * @brief Ask network to make computations on specific target device.\n",
      " |      .   * @param[in] targetId target identifier.\n",
      " |      .   * @see Target\n",
      " |      .   *\n",
      " |      .   * List of supported combinations backend / target:\n",
      " |      .   * |                        | DNN_BACKEND_OPENCV | DNN_BACKEND_INFERENCE_ENGINE | DNN_BACKEND_HALIDE |\n",
      " |      .   * |------------------------|--------------------|------------------------------|--------------------|\n",
      " |      .   * | DNN_TARGET_CPU         |                  + |                            + |                  + |\n",
      " |      .   * | DNN_TARGET_OPENCL      |                  + |                            + |                  + |\n",
      " |      .   * | DNN_TARGET_OPENCL_FP16 |                  + |                            + |                    |\n",
      " |      .   * | DNN_TARGET_MYRIAD      |                    |                            + |                    |\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  readFromModelOptimizer(...)\n",
      " |      readFromModelOptimizer(xml, bin) -> retval\n",
      " |      .   @brief Create a network from Intel's Model Optimizer intermediate representation.\n",
      " |      .   *  @param[in] xml XML configuration file with network's topology.\n",
      " |      .   *  @param[in] bin Binary file with trained weights.\n",
      " |      .   *  Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine\n",
      " |      .   *  backend.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "#Cropped=[]\n",
    "TrueIndices=[]\n",
    "Confidences=[]\n",
    "pos={}\n",
    "for i in np.arange(0, detections.shape[2]):\n",
    "    # extract the confidence (i.e., probability) associated with the\n",
    "    # prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    # filter out weak detections by ensuring the `confidence` is\n",
    "    # greater than the minimum confidence\n",
    "    idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "    if confidence > 0.5 and idx==3:\n",
    "        # extract the index of the class label from the `detections`,\n",
    "        # then compute the (x, y)-coordinates of the bounding box for\n",
    "        # the object\n",
    "        ImageIndex=int(detections[0,0,i,0])\n",
    "        w=Sizes[ImageIndex][1]    \n",
    "        h=Sizes[ImageIndex][0] \n",
    "        TrueIndices.append(ImageIndices[ImageIndex])\n",
    "        Confidences.append(confidence)\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "   \n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        #crop_img = Images[ImageIndex][startY:endY, startX:endX,:]\n",
    "        #Cropped.append(crop_img)\n",
    "        #cv2.imwrite(\"Cropped/crop{}.jpeg\".format(i),crop_img)\n",
    "        # display the prediction\n",
    "        label = \"bird{}: {:.2f}%\".format(TrueIndex,confidence * 100)\n",
    "  \n",
    "        cv2.rectangle(Images[ImageIndex], (startX, startY), (endX, endY),\n",
    "            COLORS[idx], 2)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        pos[ImageIndices[ImageIndex]]=(startX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Detected\")\n",
    "os.makedirs(\"Detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,image in enumerate(Images):\n",
    "    cv2.imwrite(\"Detected/Output{}.jpeg\".format(ImageIndices[index]), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropped=[cv2.resize(image,(224,224)) for image in Cropped]\n",
    "#Cropped=np.stack(Cropped,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x,y) for x,y in zip(TrueIndices,Confidences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Detected\")\n",
    "os.makedirs(\"Detected\")\n",
    "for index,image in enumerate(Images):\n",
    "    if ImageIndices[index] in preds:\n",
    "        print ImageIndices[index],pos[ImageIndices[index]],image.shape\n",
    "        cv2.putText(image, preds[ImageIndices[index]],pos[ImageIndices[index]] , cv2.FONT_HERSHEY_SIMPLEX, 2, COLORS[idx], 2)\n",
    "    cv2.imwrite(\"Detected/Output{}.jpeg\".format(ImageIndices[index]), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
